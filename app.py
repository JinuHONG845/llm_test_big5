import streamlit as st
import pandas as pd
import numpy as np
import json
import openai
import anthropic
import google.generativeai as genai
import time
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import random  # íŒŒì¼ ìƒë‹¨ì— ì¶”ê°€

# ì•± ê¸°ë³¸ ì„¤ì •
st.set_page_config(
    page_title="Big 5 ì„±ê²© ê²€ì‚¬",
    page_icon="ğŸ§ª",
    layout="wide"
)

# ì „ì—­ ë³€ìˆ˜ë¡œ test_mode ì„¤ì • ì œê±°
# test_mode = "ì „ì²´ í…ŒìŠ¤íŠ¸ (ë¶„í•  ì‹¤í–‰)"  # ì´ ì¤„ ì‚­ì œ

# ì—¬ë°± ì¤„ì •ì„ ìœ„í•œ CSS
st.markdown("""
    <style>
        .block-container {
            padding-top: 1rem;
            padding-bottom: 1rem;
            padding-left: 3rem;
            padding-right: 3rem;
            max-width: 100rem;
        }
        .element-container {
            margin-bottom: 1.5rem;
        }
        .stDataFrame {
            width: 100%;
        }
        .dataframe {
            margin-top: 1rem;
            margin-bottom: 2rem;
            font-size: 14px;
        }
        table {
            border-collapse: collapse;
            border-spacing: 0;
            width: 100%;
        }
        th {
            background-color: #f0f2f6;
            font-weight: bold;
            padding: 12px 8px !important;
        }
        td {
            padding: 10px 8px !important;
        }
        h3 {
            margin-top: 2rem !important;
            margin-bottom: 1rem !important;
            padding: 0.5rem 0;
            border-bottom: 2px solid #f0f2f6;
            color: #0e1117;
        }
        .stProgress > div > div {
            background-color: #00cc99;
        }
    </style>
""", unsafe_allow_html=True)

# ê¸°ë³¸ íƒ€ì´í‹€ ì„¤ì •
st.title("LLM Big 5 Test")

# JSON íŒŒì¼ë“¤ ë¡œë“œ
try:
    with open('persona.json', 'r') as f:
        personas = json.load(f)
    with open('IPIP.json', 'r') as f:
        ipip_questions = json.load(f)
    with open('BFI.json', 'r') as f:
        bfi_questions = json.load(f)
except FileNotFoundError as e:
    st.error(f"í•„ìš”í•œ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
    st.stop()
except json.JSONDecodeError as e:
    st.error(f"JSON íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
    st.stop()

# LLM ì„ íƒ ë° ì„¤ì • ë¶€ë¶„ì„ ì‚¬ì´ë“œë°”ë¡œ ì´ë™
with st.sidebar:
    st.title("í…ŒìŠ¤íŠ¸ ì„¤ì •")
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„ íƒ ì¶”ê°€
    test_mode = st.radio(
        "í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„ íƒ",
        ("í˜ë¥´ì†Œë‚˜ í…ŒìŠ¤íŠ¸", "ëŒ€ì¡°êµ° í…ŒìŠ¤íŠ¸"),
        help="í˜ë¥´ì†Œë‚˜ í…ŒìŠ¤íŠ¸: ì •ì˜ëœ í˜ë¥´ì†Œë‚˜ë¡œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰\nëŒ€ì¡°êµ° í…ŒìŠ¤íŠ¸: LLMì´ ììœ¨ì ìœ¼ë¡œ ì‘ë‹µ"
    )
    
    st.title("LLM ì„¤ì •")
    llm_choice = st.radio(
        "LLM ì„ íƒ",
        ("GPT", "Claude", "Gemini"),
        horizontal=True
    )

    # LLM ì„ íƒì— ë”°ë¥¸ ì„¸ë¶€ ëª¨ë¸ ì„ íƒ
    if llm_choice == "GPT":
        model_choice = st.radio(
            "GPT ëª¨ë¸ ì„ íƒ",
            ("GPT-4 Turbo", "GPT-3.5 Turbo"),
            horizontal=True,
            help="GPT-4 TurboëŠ” ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦½ë‹ˆë‹¤. GPT-3.5 TurboëŠ” ë” ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
    elif llm_choice == "Claude":
        model_choice = st.radio(
            "Claude ëª¨ë¸ ì„ íƒ",
            ("Claude 3 Sonnet", "Claude 3 Haiku"),
            horizontal=True,
            help="Sonnetì€ ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦½ë‹ˆë‹¤. HaikuëŠ” ë” ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
    else:  # Gemini
        model_choice = st.radio(
            "Gemini ëª¨ë¸ ì„ íƒ",
            ("Gemini Pro",),  # ë‹¨ì¼ ì˜µì…˜
            horizontal=True,
            help="í˜„ì¬ Gemini Pro ëª¨ë¸ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        )

# API í‚¤ ì„¤ì •
if llm_choice == "GPT":
    api_key = st.secrets.get("OPENAI_API_KEY")
    if not api_key:
        st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
    openai.api_key = api_key
elif llm_choice == "Claude":
    api_key = st.secrets.get("ANTHROPIC_API_KEY")
    if not api_key:
        st.error("Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
    client = anthropic.Anthropic(api_key=api_key)
else:  # Gemini
    api_key = st.secrets.get("GOOGLE_API_KEY")
    if not api_key:
        st.error("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
    genai.configure(api_key=api_key)

@retry(
    stop=stop_after_attempt(5),  # ìµœëŒ€ 5ë²ˆ ì¬ì‹œë„
    wait=wait_exponential(multiplier=1, min=4, max=20),  # 4~20ì´ˆ ì‚¬ì´ ëŒ€ê¸°ì‹œê°„
    retry=(
        retry_if_exception_type(openai.APIError) |
        retry_if_exception_type(openai.APIConnectionError) |
        retry_if_exception_type(openai.RateLimitError) |
        retry_if_exception_type(anthropic.APIError) |
        retry_if_exception_type(anthropic.APIConnectionError) |
        retry_if_exception_type(anthropic.RateLimitError)
    )
)
def get_llm_response(persona, questions, test_type):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ í˜ìŠ¤íŠ¸ ì‘ë‹µì„ ìƒì„±"""
    try:
        # ì§ˆë¬¸ ëª©ë¡ ì¤€ë¹„
        if test_type == 'IPIP':
            question_list = [q['item'] for q in questions]
            scale_description = """1 = Very inaccurate
2 = Moderately inaccurate
3 = Neither
4 = Moderately accurate
5 = Very accurate"""
        else:  # BFI
            question_list = [q['question'] for q in questions]
            scale_description = """1 = Disagree Strongly
2 = Disagree a little
3 = Neither agree nor disagree
4 = Agree a little
5 = Agree strongly"""
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± - í…ŒìŠ¤íŠ¸ ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥´ê²Œ
        if test_mode == "í˜ë¥´ì†Œë‚˜ í…ŒìŠ¤íŠ¸":
            prompt = f"""Based on this persona: {', '.join(persona['personality'])}

For each question, provide a rating from 1-5 where:
{scale_description}"""
        else:  # ëŒ€ì¡°êµ° í…ŒìŠ¤íŠ¸
            prompt = f"""As an AI, please answer these personality test questions honestly.
Rate each question from 1-5 where:
{scale_description}"""

        prompt += f"""

Return ONLY a JSON object in this exact format:
{{
    "responses": [
        {{"question": "<question text>", "score": <1-5>}},
        ...
    ]
}}

Questions to rate:
{json.dumps(question_list, indent=2)}"""
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if llm_choice == "GPT":
                    model_name = "gpt-4-turbo-preview" if model_choice == "GPT-4 Turbo" else "gpt-3.5-turbo"
                    response = openai.chat.completions.create(
                        model=model_name,
                        messages=[
                            {"role": "system", "content": "You are a helpful assistant that responds only in valid JSON format."},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=1.0,
                        timeout=30
                    )
                    content = response.choices[0].message.content
                    
                elif llm_choice == "Claude":
                    model_name = "claude-3-sonnet-20240229" if model_choice == "Claude 3 Sonnet" else "claude-3-haiku-20240307"
                    response = client.messages.create(
                        model=model_name,
                        max_tokens=2000,
                        system="You are a helpful assistant that responds only in valid JSON format.",
                        messages=[{"role": "user", "content": prompt}],
                        temperature=1.0,
                        timeout=30
                    )
                    content = response.content[0].text
                    
                else:  # Gemini
                    model_name = 'gemini-pro' if model_choice == "Gemini Pro" else 'gemini-nano'
                    model = genai.GenerativeModel(model_name)
                    response = model.generate_content(
                        prompt,
                        generation_config=genai.types.GenerationConfig(
                            temperature=1.0
                        )
                    )
                    content = response.text
                
                # JSON íŒŒì‹± ë° ê²€ì¦
                if content.startswith('```') and content.endswith('```'):
                    content = content.split('```')[1]
                    if content.startswith('json'):
                        content = content[4:]
                
                result = json.loads(content.strip())
                
                # ì‘ë‹µ ê²€ì¦
                if not result or 'responses' not in result:
                    raise ValueError("Invalid response format")
                if len(result['responses']) != len(question_list):
                    raise ValueError("Incomplete response")
                
                time.sleep(2)  # API í˜¸ì¶œ ì‚¬ì´ì— 2ì´ˆ ëŒ€ê¸°
                return result
                
            except (json.JSONDecodeError, ValueError) as e:
                if attempt == max_retries - 1:
                    st.error(f"ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    raise e
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                continue
                
            except Exception as e:
                st.error(f"LLM API ì˜¤ë¥˜: {str(e)}")
                if attempt == max_retries - 1:
                    raise e
                time.sleep(2 ** attempt)
                continue
                
    except Exception as e:
        st.error(f"ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise e

# ë°°ì¹˜ í¬ê¸° ì¡°ì • (ëª¨ë¸ì— ë”°ë¼)
def get_batch_size(model):
    if model in ["GPT-4 Turbo", "Claude 3 Sonnet", "Gemini Pro"]:
        return 25, 5  # IPIP ë°°ì¹˜ í¬ê¸°, BFI ë°°ì¹˜ í¬ê¸° (ë” ì‘ê²Œ ì¡°ì •)
    else:
        return 10, 3  # ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'accumulated_results' not in st.session_state:
    st.session_state.accumulated_results = {
        'ipip': pd.DataFrame(),
        'bfi': pd.DataFrame(),
        'completed_batches': set()
    }

# IPIP í…ŒìŠ¤íŠ¸ ì„¹ì…˜ (test_mode ì¡°ê±´ ì œê±°)
st.write("### IPIP í˜ë¥´ì†Œë‚˜ ë°°ì¹˜ ì„ íƒ")
col1, col2, col3, col4, col5 = st.columns(5)

with col1:
    ipip_batch1 = st.button("IPIP 1-10ë²ˆ", 
                      disabled='ipip_batch1' in st.session_state.accumulated_results['completed_batches'])
with col2:
    ipip_batch2 = st.button("IPIP 11-20ë²ˆ", 
                      disabled='ipip_batch2' in st.session_state.accumulated_results['completed_batches'])
with col3:
    ipip_batch3 = st.button("IPIP 21-30ë²ˆ", 
                      disabled='ipip_batch3' in st.session_state.accumulated_results['completed_batches'])
with col4:
    ipip_batch4 = st.button("IPIP 31-40ë²ˆ", 
                      disabled='ipip_batch4' in st.session_state.accumulated_results['completed_batches'])
with col5:
    ipip_batch5 = st.button("IPIP 41-50ë²ˆ", 
                      disabled='ipip_batch5' in st.session_state.accumulated_results['completed_batches'])

# BFI í…ŒìŠ¤íŠ¸ ì„¹ì…˜
st.write("### BFI í˜ë¥´ì†Œë‚˜ ë°°ì¹˜ ì„ íƒ")
col1, col2, col3, col4, col5 = st.columns(5)

with col1:
    bfi_batch1 = st.button("BFI Dummy 1-9", 
                      disabled='bfi_batch1' in st.session_state.accumulated_results['completed_batches'])
with col2:
    bfi_batch2 = st.button("BFI Dummy 10-18", 
                      disabled='bfi_batch2' in st.session_state.accumulated_results['completed_batches'])
with col3:
    bfi_batch3 = st.button("BFI Dummy 19-27", 
                      disabled='bfi_batch3' in st.session_state.accumulated_results['completed_batches'])
with col4:
    bfi_batch4 = st.button("BFI Dummy 28-36", 
                      disabled='bfi_batch4' in st.session_state.accumulated_results['completed_batches'])
with col5:
    bfi_batch5 = st.button("BFI Dummy 37-44", 
                      disabled='bfi_batch5' in st.session_state.accumulated_results['completed_batches'])

# ì´ˆê¸°í™” ë²„íŠ¼
if st.button("í…ŒìŠ¤íŠ¸ ì´ˆê¸°í™”"):
    st.session_state.accumulated_results = {
        'ipip': pd.DataFrame(),
        'bfi': pd.DataFrame(),
        'completed_batches': set()
    }
    st.rerun()

def run_batch_test(batch_name, start_idx, end_idx, test_type='IPIP'):
    ipip_batch_size, bfi_batch_size = get_batch_size(model_choice)
    
    if test_mode == "í˜ë¥´ì†Œë‚˜ í…ŒìŠ¤íŠ¸":
        batch_personas = personas[start_idx:end_idx]
        index_prefix = "Persona"
    else:  # ëŒ€ì¡°êµ° í…ŒìŠ¤íŠ¸
        batch_personas = [{"personality": ["AI Baseline Test"]} for _ in range(end_idx - start_idx)]
        index_prefix = "Dummy"

    # DataFrame ì´ˆê¸°í™” ë˜ëŠ” ê¸°ì¡´ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°
    if st.session_state.accumulated_results['ipip'].empty:
        ipip_df = pd.DataFrame(
            np.nan, 
            index=[f"{index_prefix} {i+1}" for i in range(len(personas))] + ['Average'],
            columns=[f"Q{i+1}" for i in range(300)]
        )
    else:
        ipip_df = st.session_state.accumulated_results['ipip'].copy()
    
    if st.session_state.accumulated_results['bfi'].empty:
        bfi_df = pd.DataFrame(
            np.nan, 
            index=[f"{index_prefix} {i+1}" for i in range(len(personas))] + ['Average'],
            columns=[f"Q{i+1}" for i in range(44)]  # BFIëŠ” 44ë¬¸í•­
        )
    else:
        bfi_df = st.session_state.accumulated_results['bfi'].copy()

    if test_type == 'IPIP':
        st.write("### IPIP í…ŒìŠ¤íŠ¸ ì§„í–‰ ìƒí™©")
        progress_bar = st.progress(0)
        result_table = st.empty()
        
        # IPIP í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        for i, persona in enumerate(batch_personas, start=start_idx):
            # IPIP í…ŒìŠ¤íŠ¸
            all_ipip_scores = []
            for j in range(0, 300, ipip_batch_size):
                try:
                    batch_end = min(j + ipip_batch_size, 300)
                    batch_questions = ipip_questions['items'][j:batch_end]
                    
                    ipip_responses = get_llm_response(persona, batch_questions, 'IPIP')
                    if ipip_responses and 'responses' in ipip_responses:
                        scores = [r['score'] for r in ipip_responses['responses']]
                        all_ipip_scores.extend(scores)
                        
                        current_scores = ipip_df.iloc[i].copy()
                        current_scores[j:j+len(scores)] = scores
                        ipip_df.iloc[i] = current_scores
                        ipip_df.loc['Average'] = ipip_df.iloc[:-1].mean()
                        
                        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ (1.0ì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ìˆ˜ì •)
                        progress = min(1.0, ((i - start_idx) * 300 + j + len(scores)) / (len(batch_personas) * 300))
                        progress_bar.progress(progress)
                        
                        # DataFrame ì—…ë°ì´íŠ¸
                        result_table.dataframe(
                            ipip_df.fillna(0).round().astype(int).style
                                .background_gradient(cmap='YlOrRd', vmin=1, vmax=5)
                                .format("{:d}")
                                .set_properties(**{
                                    'width': '40px',
                                    'text-align': 'center',
                                    'font-size': '13px',
                                    'border': '1px solid #e6e6e6'
                                })
                                .set_table_styles([
                                    {'selector': 'th', 'props': [
                                        ('background-color', '#f0f2f6'),
                                        ('color', '#0e1117'),
                                        ('font-weight', 'bold'),
                                        ('text-align', 'center')
                                    ]},
                                    {'selector': 'td', 'props': [
                                        ('text-align', 'center')
                                    ]},
                                    {'selector': 'table', 'props': [
                                        ('width', '100%'),
                                        ('margin', '0 auto')
                                    ]}
                                ]),
                            use_container_width=True
                        )
                        
                        time.sleep(1)  # ì‹œê°ì  íš¨ê³¼ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                        
                except Exception as e:
                    st.error(f"IPIP í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜ (í˜ë¥´ì†Œë‚˜ {i+1}, ë¬¸í•­ {j}-{batch_end}): {str(e)}")
                    continue
                
    else:  # BFI
        st.write("### BFI í…ŒìŠ¤íŠ¸ ì§„í–‰ ìƒí™©")
        progress_bar = st.progress(0)
        result_table = st.empty()
        
        total_questions = 44
        max_retries = 3
        
        # BFI í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        for i, persona in enumerate(batch_personas, start=start_idx):
            all_bfi_scores = []
            current_question = 0
            
            while current_question < total_questions:
                batch_end = min(current_question + bfi_batch_size, total_questions)
                retry_count = 0
                
                while retry_count < max_retries:
                    try:
                        current_batch = bfi_questions[current_question:batch_end]
                        bfi_responses = get_llm_response(persona, current_batch, 'BFI')
                        
                        if bfi_responses and 'responses' in bfi_responses:
                            scores = [r['score'] for r in bfi_responses['responses']]
                            
                            if len(scores) == len(current_batch):
                                # ê° ì ìˆ˜ë¥¼ ê°œë³„ì ìœ¼ë¡œ í• ë‹¹
                                for idx, score in enumerate(scores):
                                    col_name = f"Q{current_question+idx+1}"
                                    bfi_df.at[f"{index_prefix} {i+1}", col_name] = score
                                
                                all_bfi_scores.extend(scores)
                                
                                # í‰ê·  ì—…ë°ì´íŠ¸
                                bfi_df.loc['Average'] = bfi_df.iloc[:-1].mean()
                                
                                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                                progress = min(1.0, ((i - start_idx) * total_questions + len(all_bfi_scores)) / 
                                            ((end_idx - start_idx) * total_questions))
                                progress_bar.progress(progress)
                                
                                # DataFrame ì—…ë°ì´íŠ¸
                                result_table.dataframe(
                                    bfi_df.fillna(0).round().astype(int).style
                                        .background_gradient(cmap='YlOrRd', vmin=1, vmax=5)
                                        .format("{:d}"),
                                    use_container_width=True
                                )
                                
                                time.sleep(2)  # API í˜¸ì¶œ ê°„ê²© ëŠ˜ë¦¼
                                break  # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë¨
                                
                            else:
                                raise ValueError(f"ì‘ë‹µ ìˆ˜ ë¶ˆì¼ì¹˜: ì˜ˆìƒ {len(current_batch)}, ì‹¤ì œ {len(scores)}")
                                
                        else:
                            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ë‹µ í˜•ì‹")
                            
                    except Exception as e:
                        retry_count += 1
                        st.warning(f"ì¬ì‹œë„ {retry_count}/{max_retries} - {str(e)}")
                        time.sleep(2 * retry_count)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                        
                if retry_count == max_retries:
                    st.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ - {index_prefix} {i+1}, ë¬¸í•­ {current_question+1}-{batch_end}")
                    # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê³  ë‹¤ìŒìœ¼ë¡œ ì§„í–‰
                
                current_question = batch_end  # ë‹¤ìŒ ë°°ì¹˜ë¡œ ì´ë™

    # ê²°ê³¼ ëˆ„ì  ì €ì¥
    st.session_state.accumulated_results['ipip'] = ipip_df
    st.session_state.accumulated_results['bfi'] = bfi_df
    st.session_state.accumulated_results['completed_batches'].add(batch_name)

    return ipip_df, bfi_df

# ë°°ì¹˜ ë²„íŠ¼ í´ë¦­ ì²˜ë¦¬
if ipip_batch1:
    ipip_df, _ = run_batch_test('ipip_batch1', 0, 10, test_type='IPIP')
elif ipip_batch2:
    ipip_df, _ = run_batch_test('ipip_batch2', 10, 20, test_type='IPIP')
elif ipip_batch3:
    ipip_df, _ = run_batch_test('ipip_batch3', 20, 30, test_type='IPIP')
elif ipip_batch4:
    ipip_df, _ = run_batch_test('ipip_batch4', 30, 40, test_type='IPIP')
elif ipip_batch5:
    ipip_df, _ = run_batch_test('ipip_batch5', 40, 50, test_type='IPIP')
elif bfi_batch1:
    _, bfi_df = run_batch_test('bfi_batch1', 0, 9, test_type='BFI')
elif bfi_batch2:
    _, bfi_df = run_batch_test('bfi_batch2', 9, 18, test_type='BFI')
elif bfi_batch3:
    _, bfi_df = run_batch_test('bfi_batch3', 18, 27, test_type='BFI')
elif bfi_batch4:
    _, bfi_df = run_batch_test('bfi_batch4', 27, 36, test_type='BFI')
elif bfi_batch5:
    _, bfi_df = run_batch_test('bfi_batch5', 36, 44, test_type='BFI')

# CSV íŒŒì¼ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ ë¶€ë¶„
if not st.session_state.accumulated_results['ipip'].empty:
    csv_data = pd.concat([
        st.session_state.accumulated_results['ipip'].add_prefix('IPIP_Q'),
        st.session_state.accumulated_results['bfi'].add_prefix('BFI_Q')
    ], axis=1)
    
    st.download_button(
        label="ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (CSV)",
        data=csv_data.to_csv(index=True, float_format='%.10f'),
        file_name="personality_test_results.csv",
        mime="text/csv"
    )